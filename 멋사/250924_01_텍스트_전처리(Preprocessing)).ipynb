{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **[250924] 01. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEpC2jiX9tBU"
      },
      "source": [
        "## ë¬¸ì¥ í† í°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bolLsZp29iht",
        "outputId": "845c7a53-d077-4ad1-8692-15819d9dc7d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë¬¸ì¥ ë¶„ë¦¬ ê²°ê³¼: ['Hello?', 'Welcome to Text Preprocessing Session.', 'Today, we gonna learn about NLP.', '']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello? Welcome to Text Preprocessing Session. Today, we gonna learn about NLP.\"\n",
        "sentences = re.split(r'(?<=[?.])\\s*', text)\n",
        "\n",
        "print(\"ë¬¸ì¥ ë¶„ë¦¬ ê²°ê³¼:\", sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHj3g07R9xlP",
        "outputId": "a68bff3d-adec-4c0c-e4e2-d704dbb55058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë¬¸ì¥ ë¶„ë¦¬ ê²°ê³¼: ['Dr.', 'Smith is on the vacation.', 'Please contact him at test.', 'mail@example.', 'com for further details.', '']\n"
          ]
        }
      ],
      "source": [
        "text = \"Dr. Smith is on the vacation. Please contact him at test.mail@example.com for further details.\"\n",
        "sentences = re.split(r'(?<=[?.])\\s*', text)\n",
        "\n",
        "print(\"ë¬¸ì¥ ë¶„ë¦¬ ê²°ê³¼:\", sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ncvGYmU9zfY",
        "outputId": "a820211b-f76f-4fda-efc8-5ff50e374f19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë¬¸ì¥ ë¶„ë¦¬ ê²°ê³¼: ['Dr. Smith is on the vacation.', 'Please contact him at test.mail@example.com for further details.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"Dr. Smith is on the vacation. Please contact him at test.mail@example.com for further details.\"\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"ë¬¸ì¥ ë¶„ë¦¬ ê²°ê³¼:\", sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDRxvmBS_xUd"
      },
      "source": [
        "NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qDal8ptB90sB"
      },
      "outputs": [],
      "source": [
        "# print(dir(nltk))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCWqTy65ADME"
      },
      "source": [
        "## ë‹¨ì–´ í† í°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K_62D2O96am",
        "outputId": "cdb38616-60e1-4d13-ce34-7a1b68dcf7d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë‹¨ì–´ í† í°í™” ê²°ê³¼: ['NLTK', 'is', 'a', 'tool', 'that', 'can', 'preprocess', 'natural', 'language', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"NLTK is a tool that can preprocess natural language.\"\n",
        "\n",
        "# ë‹¨ì–´ í† í°í™”\n",
        "words = word_tokenize(text)\n",
        "print(\"ë‹¨ì–´ í† í°í™” ê²°ê³¼:\", words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiTiqZjRAGGH",
        "outputId": "a079ed28-0566-448b-90e7-b381a2d5b850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ê¸€ì í† í°í™” ê²°ê³¼: ['ì•ˆ', 'ë…•', 'í•˜', 'ì„¸', 'ìš”', '?', ' ', 'í…', 'ìŠ¤', 'íŠ¸', ' ', 'ì „', 'ì²˜', 'ë¦¬', ' ', 'ì‹œ', 'ê°„', 'ì—', ' ', 'ì˜¤', 'ì‹ ', ' ', 'ê²ƒ', 'ì„', ' ', 'í™˜', 'ì˜', 'í•©', 'ë‹ˆ', 'ë‹¤', '.']\n"
          ]
        }
      ],
      "source": [
        "text = \"ì•ˆë…•í•˜ì„¸ìš”? í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹œê°„ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤.\"\n",
        "\n",
        "characters = list(text)\n",
        "print(\"ê¸€ì í† í°í™” ê²°ê³¼:\", characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9pybr9yAHcm",
        "outputId": "a2439965-64b9-44c7-9562-a9ecc589fef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m495.9/495.9 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.6.0 konlpy-0.6.0\n",
            "í˜•íƒœì†Œ í† í°í™” ê²°ê³¼: ['ì•ˆë…•í•˜ì„¸ìš”', '?', 'í…ìŠ¤íŠ¸', 'ì „', 'ì²˜ë¦¬', 'ì‹œê°„', 'ì—', 'ì˜¤ì‹ ', 'ê²ƒ', 'ì„', 'í™˜ì˜', 'í•©ë‹ˆë‹¤', '.']\n",
            "í˜•íƒœì†Œ í† í°í™” ê²°ê³¼(ì–´ê°„ ì¶”ì¶œ): ['ì•ˆë…•í•˜ë‹¤', '?', 'í…ìŠ¤íŠ¸', 'ì „', 'ì²˜ë¦¬', 'ì‹œê°„', 'ì—', 'ì˜¤ì‹ ', 'ê²ƒ', 'ì„', 'í™˜ì˜', 'í•˜ë‹¤', '.']\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "text = \"ì•ˆë…•í•˜ì„¸ìš”? í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹œê°„ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤.\"\n",
        "\n",
        "morphs = okt.morphs(text)\n",
        "morphs_stem = okt.morphs(text, stem=True)\n",
        "print(\"í˜•íƒœì†Œ í† í°í™” ê²°ê³¼:\", morphs)\n",
        "print(\"í˜•íƒœì†Œ í† í°í™” ê²°ê³¼(ì–´ê°„ ì¶”ì¶œ):\", morphs_stem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUisB3ncAI_g",
        "outputId": "d6c9296b-f092-4b55-a3af-af37d7c7b96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì–´ì ˆ ì¶”ì¶œ ê²°ê³¼: ['í…ìŠ¤íŠ¸', 'í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬', 'í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹œê°„', 'ì˜¤ì‹ ', 'ì˜¤ì‹  ê²ƒ', 'í™˜ì˜', 'ì „ì²˜ë¦¬', 'ì‹œê°„']\n",
            "í’ˆì‚¬ íƒœê¹… ê²°ê³¼: ['ì•ˆë…•í•˜ì„¸ìš”/Adjective', '?/Punctuation', 'í…ìŠ¤íŠ¸/Noun', 'ì „/Modifier', 'ì²˜ë¦¬/Noun', 'ì‹œê°„/Noun', 'ì—/Josa', 'ì˜¤ì‹ /Noun', 'ê²ƒ/Noun', 'ì„/Josa', 'í™˜ì˜/Noun', 'í•©ë‹ˆë‹¤/Verb', './Punctuation']\n",
            "ëª…ì‚¬ ì¶”ì¶œ ê²°ê³¼: ['í…ìŠ¤íŠ¸', 'ì²˜ë¦¬', 'ì‹œê°„', 'ì˜¤ì‹ ', 'ê²ƒ', 'í™˜ì˜']\n"
          ]
        }
      ],
      "source": [
        "phrases = okt.phrases(text)\n",
        "pos = okt.pos(text, join = True)\n",
        "nouns = okt.nouns(text)\n",
        "\n",
        "print(\"ì–´ì ˆ ì¶”ì¶œ ê²°ê³¼:\", phrases)\n",
        "print(\"í’ˆì‚¬ íƒœê¹… ê²°ê³¼:\", pos)\n",
        "print(\"ëª…ì‚¬ ì¶”ì¶œ ê²°ê³¼:\", nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrvtqVeJAL6U"
      },
      "source": [
        "## ì´ëª¨ì§€ ë° ê¸°í˜¸ ì²˜ë¦¬\n",
        "ê·¸ëŒ€ë¡œ ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX_YQa27AKKx",
        "outputId": "5a92d293-05ae-4a2e-b9e4-b2babefa11a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'ì•ˆ' : U+C548\n",
            "'ë…•' : U+B155\n",
            "'í•˜' : U+D558\n",
            "'ì„¸' : U+C138\n",
            "'ìš”' : U+C694\n",
            "'~' : U+007E\n",
            "'~' : U+007E\n",
            "'ğŸ˜Š' : U+1F60A\n"
          ]
        }
      ],
      "source": [
        "def trans_unicode(sentence):\n",
        "    for char in sentence:\n",
        "        print(f\"'{char}' : U+{ord(char):04X}\")\n",
        "\n",
        "# ì˜ˆì‹œ ë¬¸ì¥\n",
        "sentence = \"ì•ˆë…•í•˜ì„¸ìš”~~ğŸ˜Š\"\n",
        "trans_unicode(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTRbFqitAUCU"
      },
      "source": [
        "ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOl8hMyWAO0d",
        "outputId": "606d77a7-3001-48cf-83b6-46f5f8d86023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"ì•ˆë…•í•˜ì„¸ìš”~~ğŸ˜Š\"\n",
        "cleaned_text = re.sub(r\"[^ê°€-í£a-zA-Z0-9]\", \"\", text)\n",
        "\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w6-GVmPAUiu",
        "outputId": "d888da5f-23a9-41dd-c732-8ef98a614c30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/608.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m604.2/608.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.15.0\n",
            "ì´ëª¨ì§€ ì œê±° ê²°ê³¼: ì•ˆë…•í•˜ì„¸ìš”~~\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "import emoji\n",
        "\n",
        "text = \"ì•ˆë…•í•˜ì„¸ìš”~~ğŸ˜Š\"\n",
        "no_emoji_text = emoji.replace_emoji(text, replace=\"\")\n",
        "\n",
        "print(\"ì´ëª¨ì§€ ì œê±° ê²°ê³¼:\", no_emoji_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxbSsmMGAc0k"
      },
      "source": [
        "ë³€í™˜í•˜ì—¬ ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ngKRbTLAWde",
        "outputId": "b4947429-7d12-4e36-e892-dad47eb3b104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”~~:smiling_face_with_smiling_eyes:\n"
          ]
        }
      ],
      "source": [
        "text = \"ì•ˆë…•í•˜ì„¸ìš”~~ğŸ˜Š\"\n",
        "\n",
        "converted_text = emoji.demojize(text)\n",
        "print(converted_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRlH-mAYAfge",
        "outputId": "14677366-f706-4258-9059-8ee2f7669b92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”~~:ë¯¸ì†Œ_ì§“ëŠ”_ëˆˆìœ¼ë¡œ_ì‚´ì§_ì›ƒëŠ”_ì–¼êµ´:\n"
          ]
        }
      ],
      "source": [
        "text = \"ì•ˆë…•í•˜ì„¸ìš”~~ğŸ˜Š\"\n",
        "\n",
        "converted_text = emoji.demojize(text, language = \"ko\")\n",
        "print(converted_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljGSZ4dFAhBm",
        "outputId": "7aafb163-9c8e-455a-adec-0e1313917d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLPëŠ” ë„ˆë¬´ ì¬ë°Œì–´ã…‹ã…‹ã…‹\n"
          ]
        }
      ],
      "source": [
        "text = \"NLPëŠ” ë„ˆë¬´ ì¬ë°Œì–´ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹\"\n",
        "\n",
        "converted_text = re.sub(r'ã…‹{3,}', 'ã…‹ã…‹ã…‹', text)\n",
        "print(converted_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I04l06vZAjYh",
        "outputId": "48be3a80-ec55-4dac-fbbe-75ff1955c51f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì •ê·œí‘œí˜„ì‹ë„ ì¬ë°Œì–´^^\n"
          ]
        }
      ],
      "source": [
        "text = \"ì •ê·œí‘œí˜„ì‹ë„ ì¬ë°Œì–´^0^\"\n",
        "\n",
        "converted_text = re.sub(r'\\^[_\\-^0oOã…¡]+?\\^', '^^', text)\n",
        "print(converted_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTgDjfgIAlNs"
      },
      "source": [
        "## í™•ì¥ ë° ì¶•ì•½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IkhheGuAkWj",
        "outputId": "718f1bbb-bbda-4b66-ef23-56f6d6c65fa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.3 contractions-0.1.73 pyahocorasick-2.2.0 textsearch-0.0.24\n",
            "í…ìŠ¤íŠ¸ í™•ì¥ ê²°ê³¼: I cannot do this. It is not right.\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions\n",
        "from contractions import fix\n",
        "\n",
        "text = \"I can't do this. It's not right.\"\n",
        "expanded_text = fix(text)\n",
        "print(\"í…ìŠ¤íŠ¸ í™•ì¥ ê²°ê³¼:\", expanded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTCkMt37AoB_",
        "outputId": "4357d8cd-6188-44a3-b3c8-5ecee7943147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í…ìŠ¤íŠ¸ í™•ì¥ ê²°ê³¼: I cannot do this. it is not right.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "contractions_dict = {\n",
        "    \"isn't\": \"is not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"they're\": \"they are\",\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b', re.IGNORECASE)\n",
        "    expanded_text = pattern.sub(lambda x: contractions_dict[x.group().lower()], text)\n",
        "    return expanded_text\n",
        "\n",
        "text = \"I can't do this. It's not right.\"\n",
        "expanded_text = expand_contractions(text, contractions_dict)\n",
        "print(\"í…ìŠ¤íŠ¸ í™•ì¥ ê²°ê³¼:\", expanded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29WSX9TwAqaU"
      },
      "source": [
        "## ë¶ˆìš©ì–´ ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO2yR8HuApVg",
        "outputId": "3fed99e3-2664-404b-dd47-16eb8fec0350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë¶ˆìš©ì–´ ì œê±° ê²°ê³¼: ['simple', 'sentence', 'demonstrating', 'stopword', 'removal.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"This is a simple sentence for demonstrating stopword removal.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = text.split()\n",
        "\n",
        "filtered_sentence = [w for w in words if not w.lower() in stop_words]\n",
        "print(\"ë¶ˆìš©ì–´ ì œê±° ê²°ê³¼:\", filtered_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZPySuavAtMU"
      },
      "source": [
        "## ì–´ê°„ ë° í‘œì œì–´ ì¶”ì¶œ\n",
        "ì–´ê°„ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgYaZYATAr8w",
        "outputId": "0d396fe0-123f-4aa1-c88d-4588a8180df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì–´ê°„ ì¶”ì¶œ ê²°ê³¼: ['run', 'ran', 'run', 'easili', 'fairli']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"ran\", \"runs\", \"easily\", \"fairly\"]\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"ì–´ê°„ ì¶”ì¶œ ê²°ê³¼:\", stemmed_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03IzOmLAA1ms"
      },
      "source": [
        "í‘œì œì–´ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-Q2VYbzAyRC",
        "outputId": "7e93fb0f-9ce0-46e5-ab8a-46c27aeab709"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í‘œì œì–´ ì¶”ì¶œ ê²°ê³¼: ['run', 'run', 'run', 'easily', 'fairly']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"ran\", \"runs\", \"easily\", \"fairly\"]\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos=\"v\") for word in words]\n",
        "print(\"í‘œì œì–´ ì¶”ì¶œ ê²°ê³¼:\", lemmatized_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t42tl6KLA25r",
        "outputId": "43e8ec72-5508-476a-a635-a250ff68039e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì–´ê°„ ì¶”ì¶œ ê²°ê³¼: ['ë›°ì–´ë‹¤ë‹ˆë‹¤', 'ì‚¬ëŒê³¼', 'ê±¸ë‹¤', 'ë‹¤ë‹ˆë‹¤', 'ì‚¬ëŒ', 'ëª¨ë‘', 'ì±…', 'ì„', 'ì¢‹ì•„í•˜ë‹¤', '.']\n"
          ]
        }
      ],
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "text = \"ë›°ì–´ë‹¤ë‹ˆëŠ” ì‚¬ëŒê³¼ ê±¸ì–´ë‹¤ë‹ˆëŠ” ì‚¬ëŒ ëª¨ë‘ ì±…ì„ ì¢‹ì•„í•©ë‹ˆë‹¤.\"\n",
        "\n",
        "# ì–´ê°„ ì¶”ì¶œ (stem=True ì˜µì…˜ ì‚¬ìš©)\n",
        "stemmed_words = okt.morphs(text, stem=True)\n",
        "\n",
        "print(\"ì–´ê°„ ì¶”ì¶œ ê²°ê³¼:\", stemmed_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmpG9acYBNjM"
      },
      "source": [
        "## êµ¬ë¬¸ ë¶„ì„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2EeuePTBJ2n",
        "outputId": "713cce79-e096-4dcc-95f2-061123ddffd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S (NP (DT the) (NN cat)) (VP (VB chased) (NP (DT the) (NN dog))))\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import CFG\n",
        "\n",
        "# ë¬¸ë²• ì •ì˜\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  NP -> DT NN\n",
        "  VP -> VB NP\n",
        "  DT -> 'the'\n",
        "  NN -> 'cat' | 'dog'\n",
        "  VB -> 'chased' | 'saw'\n",
        "\"\"\")\n",
        "\n",
        "parser = nltk.ChartParser(grammar)\n",
        "sentence = ['the', 'cat', 'chased', 'the', 'dog']\n",
        "\n",
        "for tree in parser.parse(sentence):\n",
        "    print(tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maZ_t154BSV0"
      },
      "source": [
        "## N-gram ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko6TZ7O6BRjj",
        "outputId": "30e786fc-3f6b-4f44-cc3b-e13ee645cc62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2-ê·¸ë¨ ê²°ê³¼: [('I', 'love'), ('love', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', '.')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import ngrams\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"I love natural language processing.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "n = 2\n",
        "bigrams = list(ngrams(tokens, n))\n",
        "print(f\"{n}-ê·¸ë¨ ê²°ê³¼:\", bigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFcwpH-JBYAE"
      },
      "source": [
        "## Naive Bayes Classifier ì˜ˆì œ\n",
        "\n",
        "ì£¼ì˜.  \n",
        "accuracyê°€ 0ì´ë‚˜ 1ì¼ ê²½ìš°(ëª¨ë‘ í‹€ë¦¬ê±°ë‚˜ ëª¨ë‘ ë§ì¶˜ ê²½ìš°) ì–´ë–¤ ê²ƒì´ ì¤‘ìš”í–ˆëŠ”ì§€ ë¶„ê°„í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ì´ ê²½ìš° Most Informative Featuresê°€ ì¶œë ¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siVMTlOdBVcY",
        "outputId": "ed0c0f81-07da-4a4e-bfd2-bacc0a3311e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë¶„ë¥˜ê¸° ì •í™•ë„: 0.69\n",
            "Most Informative Features\n",
            "               ludicrous = True              neg : pos    =     15.2 : 1.0\n",
            "                  avoids = True              pos : neg    =     13.1 : 1.0\n",
            "             fascination = True              pos : neg    =     11.1 : 1.0\n",
            "             outstanding = True              pos : neg    =     10.9 : 1.0\n",
            "               animators = True              pos : neg    =     10.4 : 1.0\n",
            "                    slip = True              pos : neg    =     10.4 : 1.0\n",
            "                thematic = True              pos : neg    =     10.4 : 1.0\n",
            "               insulting = True              neg : pos    =     10.1 : 1.0\n",
            "                 conveys = True              pos : neg    =      9.8 : 1.0\n",
            "                seamless = True              pos : neg    =      9.8 : 1.0\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk import NaiveBayesClassifier\n",
        "import random\n",
        "\n",
        "# ë°ì´í„°ì…‹ ì¤€ë¹„\n",
        "nltk.download('movie_reviews')\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# ë°ì´í„° ì „ì²˜ë¦¬\n",
        "def document_features(document):\n",
        "    words = set(document)\n",
        "    features = {}\n",
        "    for word in nltk.FreqDist(words):\n",
        "        features[word] = True\n",
        "    return features\n",
        "\n",
        "random.shuffle(documents)\n",
        "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
        "\n",
        "# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬\n",
        "train_set, test_set = featuresets[100:], featuresets[:100]\n",
        "\n",
        "# NaiveBayes ë¶„ë¥˜ê¸° í›ˆë ¨\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì •í™•ë„ í‰ê°€\n",
        "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
        "print(f\"ë¶„ë¥˜ê¸° ì •í™•ë„: {accuracy:.2f}\")\n",
        "\n",
        "# ìƒìœ„ 10ê°œ ì •ë³´ëŸ‰ì´ ë†’ì€ ë‹¨ì–´\n",
        "classifier.show_most_informative_features(n = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZevzRdphiPLv"
      },
      "source": [
        "## ì •ê·œ í‘œí˜„ì‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1b-4j9NJsC4",
        "outputId": "48c6d0f7-3846-4d8e-f2ca-adf1f55b4737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì´ë©”ì¼ ì£¼ì†Œ: ['eamil1@example.com', 'eamil2@example.co.kr']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"ì¶”ê°€ì ì¸ ë¬¸ì˜ì‚¬í•­ì´ë‚˜ ê¸°íƒ€ ì˜ê²¬ì´ ìˆìœ¼ë©´ eamil1@example.com í˜¹ì€ eamil2@example.co.krìœ¼ë¡œ ì—°ë½ë°”ëë‹ˆë‹¤.\"\n",
        "\n",
        "pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "emails = re.findall(pattern, text)\n",
        "\n",
        "print(\"ì´ë©”ì¼ ì£¼ì†Œ:\", emails)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX1JQNrdiUSs",
        "outputId": "65fbc4df-10f4-45dc-d48c-347c2e8081f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë³€í™˜ëœ ë‚ ì§œ í˜•ì‹: 2024-10-20\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"20/10/2024\"\n",
        "\n",
        "# ë‚ ì§œ í˜•ì‹ì„ YYYY-MM-DDë¡œ ë³€í™˜\n",
        "new_date = re.sub(r'(\\d{2})/(\\d{2})/(\\d{4})', r'\\3-\\2-\\1', text)\n",
        "print(\"ë³€í™˜ëœ ë‚ ì§œ í˜•ì‹:\", new_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2ltQigMjGgG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
